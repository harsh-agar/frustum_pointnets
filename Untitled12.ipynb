{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsh-agar/frustum_pointnets/blob/master/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knUV6yx6MZ3D",
        "colab_type": "code",
        "outputId": "776bf8a0-41ed-4263-8510-9d680376cc89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYEGS4W0otv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9d1e1a58-692c-4524-b924-258c1b2469fa"
      },
      "source": [
        "import numpy as np\n",
        "  \n",
        "a = np.arange(5) \n",
        "  \n",
        "# a is printed. \n",
        "print(\"a is:\") \n",
        "print(a) \n",
        "  \n",
        "# the array is saved in the file geekfile.npy  \n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/train_feats', a) \n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/val_feats', a) \n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/train_labels', a) \n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/val_labels', a)\n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/train_feats', a) \n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/val_feats', a) \n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/train_labels', a) \n",
        "np.save('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/val_labels', a)\n",
        "np.save('train_feats', a) \n",
        "np.save('val_feats', a) \n",
        "np.save('train_labels', a) \n",
        "np.save('val_labels', a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a is:\n",
            "[0 1 2 3 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zitV5KxZMjId",
        "colab_type": "code",
        "outputId": "734cd43f-840c-4f48-dcc3-4a9c5044fc30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import importlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "print (\"1\")\n",
        "ROOT_DIR = '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/'\n",
        "BASE_DIR = '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/'\n",
        "sys.path.append('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/')\n",
        "sys.path.append('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/')\n",
        "sys.path.append('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/train/')\n",
        "import provider_1 as provider\n",
        "print (\"2\")\n",
        "from model_util import MODEL, LOG_DIR, NUM_POINT\n",
        "from model_util import BATCH_SIZE, NUM_POINT, MAX_EPOCH\n",
        "from model_util import LEARNING_RATE, GPU_INDEX, MOMENTUM\n",
        "from model_util import OPTIMIZER, DECAY_STEP, DECAY_RATE\n",
        "from model_util import NUM_FEATURES, NUM_CLASSES, RESTORE_MODEL_PATH\n",
        "\n",
        "NUM_FEATURES=3\n",
        "NUM_CLASSES=40\n",
        "NUM_POINT = 1024\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Set training configurations\n",
        "EPOCH_CNT = 0\n",
        "print (\"3\")\n",
        "\n",
        "MODEL_FILE = os.path.join(ROOT_DIR, 'models', MODEL+'.py')\n",
        "MODEL = importlib.import_module(MODEL) # import network module\n",
        "LOG_DIR = '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/log/'\n",
        "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
        "os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
        "os.system('cp %s %s' % (os.path.join(BASE_DIR, 'train.py'), LOG_DIR))\n",
        "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
        "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99\n",
        "\n",
        "# # Load Frustum Datasets. Use default data paths.\n",
        "# TRAIN_DATASET_FEATS = np.load('train_feats.npy')\n",
        "# TRAIN_DATASET_LABELS = np.load('train_labels.npy')\n",
        "# TEST_DATASET_FEATS = np.load('train_feats.npy')\n",
        "# TEST_DATASET_LABELS = np.load('train_labels.npy')\n",
        "\n",
        "TRAIN_FILES = provider.getDataFiles( \\\n",
        "    os.path.join(BASE_DIR, '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/data/modelnet40_ply_hdf5_2048/train_files.txt'))\n",
        "TEST_FILES = provider.getDataFiles(\\\n",
        "    os.path.join(BASE_DIR, '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/data/modelnet40_ply_hdf5_2048/test_files.txt'))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq6aOp5NNPVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_string(out_str):\n",
        "    LOG_FOUT.write(out_str+'\\n')\n",
        "    LOG_FOUT.flush()\n",
        "    print(out_str)\n",
        "\n",
        "def get_learning_rate(batch):\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "                        LEARNING_RATE,  # Base learning rate.\n",
        "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                        DECAY_STEP,          # Decay step.\n",
        "                        DECAY_RATE,          # Decay rate.\n",
        "                        staircase=True)\n",
        "    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "    return learning_rate        \n",
        "\n",
        "def get_bn_decay(batch):\n",
        "    bn_momentum = tf.train.exponential_decay(\n",
        "                      BN_INIT_DECAY,\n",
        "                      batch*BATCH_SIZE,\n",
        "                      BN_DECAY_DECAY_STEP,\n",
        "                      BN_DECAY_DECAY_RATE,\n",
        "                      staircase=True)\n",
        "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        "    return bn_decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa69K2G4PJf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_one_epoch(sess, ops, train_writer):\n",
        "    ''' Training for one epoch on the frustum dataset.\n",
        "    ops is dict mapping from string to tf ops\n",
        "    '''\n",
        "    is_training = True\n",
        "    log_string(str(datetime.now()))\n",
        "    \n",
        "    # Shuffle train samples\n",
        "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
        "    np.random.shuffle(train_file_idxs)\n",
        "#     num_batches = int(len(TRAIN_DATASET_LABELS)/BATCH_SIZE)\n",
        "\n",
        "    # To collect statistics\n",
        "    total_correct = 0\n",
        "    total_seen = 0\n",
        "    loss_sum = 0\n",
        "    label_correct_cnt = 0\n",
        "\n",
        "\n",
        "    for fn in range(len(TRAIN_FILES)):\n",
        "        log_string('----' + str(fn) + '-----')\n",
        "        current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
        "        current_data = current_data[:,0:NUM_POINT,:]\n",
        "        current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
        "        current_label = np.squeeze(current_label)\n",
        "        \n",
        "        file_size = current_data.shape[0]\n",
        "        num_batches = file_size // BATCH_SIZE\n",
        "        \n",
        "        total_correct = 0\n",
        "        total_seen = 0\n",
        "        loss_sum = 0\n",
        "       \n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
        "            \n",
        "            # Augment batched point clouds by rotation and jittering\n",
        "            rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
        "            jittered_data = provider.jitter_point_cloud(rotated_data)\n",
        "\n",
        "            feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
        "                         ops['class_pl']: current_label[start_idx:end_idx],\n",
        "                         ops['is_training_pl']: is_training}\n",
        "\n",
        "            summary, step, _, loss_val, label_logits_val = \\\n",
        "                sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'],\n",
        "                    ops['class_logits']], feed_dict=feed_dict)\n",
        "\n",
        "            train_writer.add_summary(summary, step)\n",
        "\n",
        "            label_preds_val = np.argmax(label_logits_val, 1)\n",
        "            correct = np.sum(label_preds_val == current_label[start_idx:end_idx])\n",
        "            total_correct += correct\n",
        "            total_seen += (BATCH_SIZE)\n",
        "            loss_sum += loss_val\n",
        " \n",
        "        log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
        "        log_string('accuracy: %f' % (total_correct / float(total_seen)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oM2n6XaPhHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_one_epoch(sess, ops, test_writer):\n",
        "    ''' Simple evaluation for one epoch on the frustum dataset.\n",
        "    ops is dict mapping from string to tf ops \"\"\"\n",
        "    '''\n",
        "    global EPOCH_CNT\n",
        "    is_training = False\n",
        "    log_string(str(datetime.now()))\n",
        "    log_string('---- EPOCH %03d EVALUATION ----'%(EPOCH_CNT))\n",
        "    # test_idxs = np.arange(0, len(TEST_DATASET_LABELS))\n",
        "    # num_batches = int(len(TEST_DATASET_LABELS)/BATCH_SIZE)\n",
        "\n",
        "    # To collect statistics\n",
        "    total_correct = 0\n",
        "    total_seen = 0\n",
        "    loss_sum = 0\n",
        "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
        "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
        "       \n",
        "    print(len(TEST_FILES))\n",
        "    for fn in range(len(TEST_FILES)):\n",
        "        log_string('----' + str(fn) + '-----')\n",
        "        current_data, current_label = provider.loadDataFile(TEST_FILES[fn])\n",
        "        current_data = current_data[:,0:NUM_POINT,:]\n",
        "        current_label = np.squeeze(current_label)\n",
        "        \n",
        "        file_size = current_data.shape[0]\n",
        "        num_batches = file_size // BATCH_SIZE\n",
        "        \n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
        "        print ((current_data[start_idx:end_idx, :, :]).shape)\n",
        "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
        "                     ops['class_pl']: current_label[start_idx:end_idx],\n",
        "                     ops['is_training_pl']: is_training}\n",
        "\n",
        "        summary, step, loss_val, label_logits_val = \\\n",
        "            sess.run([ops['merged'], ops['step'], ops['loss'],\n",
        "                ops['class_logits']], feed_dict=feed_dict)\n",
        "        test_writer.add_summary(summary, step)\n",
        "\n",
        "        label_preds_val = np.argmax(label_logits_val, 1)\n",
        "        correct = np.sum(label_preds_val == current_label[start_idx:end_idx])\n",
        "        total_correct += correct\n",
        "        total_seen += BATCH_SIZE\n",
        "        loss_sum += loss_val\n",
        "        for i in range(start_idx, end_idx):\n",
        "            l = current_label[i]\n",
        "            total_seen_class[l] += 1\n",
        "            total_correct_class[l] += (label_preds_val[i-start_idx] == l)\n",
        "\n",
        "\n",
        "    log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
        "    log_string('eval classification accuracy: %f'% \\\n",
        "        (total_correct / float(total_seen)))\n",
        "         \n",
        "    EPOCH_CNT += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf9dPOh9ywb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhEQ4Dc5zPIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvYZodVjIXBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8c0ff6f-4494-4da3-c00a-00946fb008bd"
      },
      "source": [
        "with tf.Graph().as_default():\n",
        "    with tf.device('/gpu:'+str(GPU_INDEX)):\n",
        "        # pointclouds_pl, class_pl = \\\n",
        "        #     MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
        "        pointclouds_pl = tf.placeholder(tf.float32,\n",
        "            shape=(BATCH_SIZE, NUM_POINT, NUM_FEATURES))\n",
        "        class_pl = tf.placeholder(tf.uint8, shape=(BATCH_SIZE,))\n",
        "        is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "\n",
        "        # Note the global_step=batch parameter to minimize. \n",
        "        # That tells the optimizer to increment the 'batch' parameter\n",
        "        # for you every time it trains.\n",
        "        batch = tf.get_variable('batch', [],\n",
        "            initializer=tf.constant_initializer(0), trainable=False)\n",
        "        bn_decay = get_bn_decay(batch)\n",
        "        tf.summary.scalar('bn_decay', bn_decay)\n",
        "\n",
        "        # Get model and losses \n",
        "        end_points = MODEL.get_model(pointclouds_pl,\n",
        "            is_training_pl, bn_decay=bn_decay)\n",
        "        print(class_pl)\n",
        "        print(\"+++++++++++++\")\n",
        "        loss = MODEL.get_loss(BATCH_SIZE, class_pl, end_points)\n",
        "        tf.summary.scalar('loss', loss)\n",
        "\n",
        "        losses = tf.get_collection('losses')\n",
        "        total_loss = tf.add_n(losses, name='total_loss')\n",
        "        tf.summary.scalar('total_loss', total_loss)\n",
        "\n",
        "        correct = tf.equal(tf.argmax(end_points['class_logits'], 1),\n",
        "            tf.to_int64(class_pl))\n",
        "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / \\\n",
        "            float(BATCH_SIZE)\n",
        "        tf.summary.scalar('classification accuracy', accuracy)\n",
        "\n",
        "        # Get training operator\n",
        "        learning_rate = get_learning_rate(batch)\n",
        "        tf.summary.scalar('learning_rate', learning_rate)\n",
        "        if OPTIMIZER == 'momentum':\n",
        "            optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
        "                momentum=MOMENTUM)\n",
        "        elif OPTIMIZER == 'adam':\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "        train_op = optimizer.minimize(loss, global_step=batch)\n",
        "\n",
        "        # Add ops to save and restore all the variables.\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "    # Create a session\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.allow_soft_placement = True\n",
        "    config.log_device_placement = False\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "    # Add summary writers\n",
        "    merged = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n",
        "    test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)\n",
        "\n",
        "    # Init variables\n",
        "    if RESTORE_MODEL_PATH is None:\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "    else:\n",
        "        saver.restore(sess, FLAGS.RESTORE_MODEL_PATH)\n",
        "\n",
        "    ops = {'pointclouds_pl': pointclouds_pl,\n",
        "           'class_pl': class_pl,\n",
        "           'is_training_pl': is_training_pl,\n",
        "           'class_logits': end_points['class_logits'],\n",
        "           'loss': loss,\n",
        "           'train_op': train_op,\n",
        "           'merged': merged,\n",
        "           'step': batch,\n",
        "           'end_points': end_points}\n",
        "\n",
        "    for epoch in range(MAX_EPOCH):\n",
        "        log_string('**** EPOCH %03d ****' % (epoch))\n",
        "        sys.stdout.flush()\n",
        "             \n",
        "        train_one_epoch(sess, ops, train_writer)\n",
        "        eval_one_epoch(sess, ops, test_writer)\n",
        "\n",
        "        # Save the variables to disk.\n",
        "        if epoch % 10 == 0:\n",
        "            save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
        "            log_string(\"Model saved in file: %s\" % save_path)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Placeholder_1:0\", shape=(32,), dtype=uint8, device=/device:GPU:0)\n",
            "+++++++++++++\n",
            "Tensor(\"Placeholder_1:0\", shape=(32,), dtype=uint8, device=/device:GPU:0)\n",
            "**** EPOCH 000 ****\n",
            "2019-08-29 11:28:31.264233\n",
            "----0-----\n",
            "mean loss: 0.144410\n",
            "accuracy: 0.119141\n",
            "----1-----\n",
            "mean loss: 0.128424\n",
            "accuracy: 0.120605\n",
            "----2-----\n",
            "mean loss: 0.123675\n",
            "accuracy: 0.135417\n",
            "----3-----\n",
            "mean loss: 0.122660\n",
            "accuracy: 0.129395\n",
            "----4-----\n",
            "mean loss: 0.120379\n",
            "accuracy: 0.130371\n",
            "2019-08-29 11:29:37.304296\n",
            "---- EPOCH 001 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007366\n",
            "eval classification accuracy: 0.062500\n",
            "Model saved in file: /content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/log/model.ckpt\n",
            "**** EPOCH 001 ****\n",
            "2019-08-29 11:29:38.428259\n",
            "----0-----\n",
            "mean loss: 0.123482\n",
            "accuracy: 0.136029\n",
            "----1-----\n",
            "mean loss: 0.123411\n",
            "accuracy: 0.142090\n",
            "----2-----\n",
            "mean loss: 0.122162\n",
            "accuracy: 0.129883\n",
            "----3-----\n",
            "mean loss: 0.116953\n",
            "accuracy: 0.132812\n",
            "----4-----\n",
            "mean loss: 0.116884\n",
            "accuracy: 0.131836\n",
            "2019-08-29 11:30:43.842360\n",
            "---- EPOCH 002 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007242\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 002 ****\n",
            "2019-08-29 11:30:44.498459\n",
            "----0-----\n",
            "mean loss: 0.116015\n",
            "accuracy: 0.134277\n",
            "----1-----\n",
            "mean loss: 0.116260\n",
            "accuracy: 0.141544\n",
            "----2-----\n",
            "mean loss: 0.118728\n",
            "accuracy: 0.132324\n",
            "----3-----\n",
            "mean loss: 0.115763\n",
            "accuracy: 0.135742\n",
            "----4-----\n",
            "mean loss: 0.122365\n",
            "accuracy: 0.141113\n",
            "2019-08-29 11:31:49.999162\n",
            "---- EPOCH 003 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007186\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 003 ****\n",
            "2019-08-29 11:31:50.661012\n",
            "----0-----\n",
            "mean loss: 0.119411\n",
            "accuracy: 0.133301\n",
            "----1-----\n",
            "mean loss: 0.120283\n",
            "accuracy: 0.144531\n",
            "----2-----\n",
            "mean loss: 0.115789\n",
            "accuracy: 0.142157\n",
            "----3-----\n",
            "mean loss: 0.114808\n",
            "accuracy: 0.132324\n",
            "----4-----\n",
            "mean loss: 0.117749\n",
            "accuracy: 0.133789\n",
            "2019-08-29 11:32:55.871249\n",
            "---- EPOCH 004 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007163\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 004 ****\n",
            "2019-08-29 11:32:56.520474\n",
            "----0-----\n",
            "mean loss: 0.117224\n",
            "accuracy: 0.135742\n",
            "----1-----\n",
            "mean loss: 0.119707\n",
            "accuracy: 0.146973\n",
            "----2-----\n",
            "mean loss: 0.114922\n",
            "accuracy: 0.136230\n",
            "----3-----\n",
            "mean loss: 0.116127\n",
            "accuracy: 0.132324\n",
            "----4-----\n",
            "mean loss: 0.115279\n",
            "accuracy: 0.144608\n",
            "2019-08-29 11:34:01.649689\n",
            "---- EPOCH 005 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007201\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 005 ****\n",
            "2019-08-29 11:34:02.288228\n",
            "----0-----\n",
            "mean loss: 0.114931\n",
            "accuracy: 0.136230\n",
            "----1-----\n",
            "mean loss: 0.118334\n",
            "accuracy: 0.134766\n",
            "----2-----\n",
            "mean loss: 0.115256\n",
            "accuracy: 0.143382\n",
            "----3-----\n",
            "mean loss: 0.115992\n",
            "accuracy: 0.133301\n",
            "----4-----\n",
            "mean loss: 0.119748\n",
            "accuracy: 0.144531\n",
            "2019-08-29 11:35:07.225237\n",
            "---- EPOCH 006 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007419\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 006 ****\n",
            "2019-08-29 11:35:07.872642\n",
            "----0-----\n",
            "mean loss: 0.114282\n",
            "accuracy: 0.134277\n",
            "----1-----\n",
            "mean loss: 0.116020\n",
            "accuracy: 0.142770\n",
            "----2-----\n",
            "mean loss: 0.118270\n",
            "accuracy: 0.132324\n",
            "----3-----\n",
            "mean loss: 0.114162\n",
            "accuracy: 0.136230\n",
            "----4-----\n",
            "mean loss: 0.120558\n",
            "accuracy: 0.144043\n",
            "2019-08-29 11:36:12.964412\n",
            "---- EPOCH 007 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007166\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 007 ****\n",
            "2019-08-29 11:36:13.623446\n",
            "----0-----\n",
            "mean loss: 0.115388\n",
            "accuracy: 0.134277\n",
            "----1-----\n",
            "mean loss: 0.118843\n",
            "accuracy: 0.144531\n",
            "----2-----\n",
            "mean loss: 0.114647\n",
            "accuracy: 0.135254\n",
            "----3-----\n",
            "mean loss: 0.113656\n",
            "accuracy: 0.143995\n",
            "----4-----\n",
            "mean loss: 0.116408\n",
            "accuracy: 0.134766\n",
            "2019-08-29 11:37:18.808796\n",
            "---- EPOCH 008 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007161\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 008 ****\n",
            "2019-08-29 11:37:19.449811\n",
            "----0-----\n",
            "mean loss: 0.113965\n",
            "accuracy: 0.137207\n",
            "----1-----\n",
            "mean loss: 0.119964\n",
            "accuracy: 0.145020\n",
            "----2-----\n",
            "mean loss: 0.116633\n",
            "accuracy: 0.134766\n",
            "----3-----\n",
            "mean loss: 0.115019\n",
            "accuracy: 0.133301\n",
            "----4-----\n",
            "mean loss: 0.114474\n",
            "accuracy: 0.142157\n",
            "2019-08-29 11:38:24.669899\n",
            "---- EPOCH 009 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007157\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 009 ****\n",
            "2019-08-29 11:38:25.320207\n",
            "----0-----\n",
            "mean loss: 0.114243\n",
            "accuracy: 0.135254\n",
            "----1-----\n",
            "mean loss: 0.115361\n",
            "accuracy: 0.143995\n",
            "----2-----\n",
            "mean loss: 0.119286\n",
            "accuracy: 0.145020\n",
            "----3-----\n",
            "mean loss: 0.115958\n",
            "accuracy: 0.137207\n",
            "----4-----\n",
            "mean loss: 0.114554\n",
            "accuracy: 0.135742\n",
            "2019-08-29 11:39:30.541005\n",
            "---- EPOCH 010 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007163\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 010 ****\n",
            "2019-08-29 11:39:31.190864\n",
            "----0-----\n",
            "mean loss: 0.115012\n",
            "accuracy: 0.135742\n",
            "----1-----\n",
            "mean loss: 0.119391\n",
            "accuracy: 0.145996\n",
            "----2-----\n",
            "mean loss: 0.113698\n",
            "accuracy: 0.142770\n",
            "----3-----\n",
            "mean loss: 0.116969\n",
            "accuracy: 0.134277\n",
            "----4-----\n",
            "mean loss: 0.114933\n",
            "accuracy: 0.134277\n",
            "2019-08-29 11:40:36.206610\n",
            "---- EPOCH 011 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007266\n",
            "eval classification accuracy: 0.062500\n",
            "Model saved in file: /content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/log/model.ckpt\n",
            "**** EPOCH 011 ****\n",
            "2019-08-29 11:40:37.053753\n",
            "----0-----\n",
            "mean loss: 0.119327\n",
            "accuracy: 0.145020\n",
            "----1-----\n",
            "mean loss: 0.115051\n",
            "accuracy: 0.137695\n",
            "----2-----\n",
            "mean loss: 0.113436\n",
            "accuracy: 0.136719\n",
            "----3-----\n",
            "mean loss: 0.114536\n",
            "accuracy: 0.136230\n",
            "----4-----\n",
            "mean loss: 0.112900\n",
            "accuracy: 0.142157\n",
            "2019-08-29 11:41:42.193018\n",
            "---- EPOCH 012 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007159\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 012 ****\n",
            "2019-08-29 11:41:42.859340\n",
            "----0-----\n",
            "mean loss: 0.119368\n",
            "accuracy: 0.146484\n",
            "----1-----\n",
            "mean loss: 0.115116\n",
            "accuracy: 0.135254\n",
            "----2-----\n",
            "mean loss: 0.113290\n",
            "accuracy: 0.137207\n",
            "----3-----\n",
            "mean loss: 0.114413\n",
            "accuracy: 0.136719\n",
            "----4-----\n",
            "mean loss: 0.114487\n",
            "accuracy: 0.144608\n",
            "2019-08-29 11:42:48.127777\n",
            "---- EPOCH 013 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007176\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 013 ****\n",
            "2019-08-29 11:42:48.769929\n",
            "----0-----\n",
            "mean loss: 0.113265\n",
            "accuracy: 0.135254\n",
            "----1-----\n",
            "mean loss: 0.119291\n",
            "accuracy: 0.146973\n",
            "----2-----\n",
            "mean loss: 0.115448\n",
            "accuracy: 0.138184\n",
            "----3-----\n",
            "mean loss: 0.115230\n",
            "accuracy: 0.134766\n",
            "----4-----\n",
            "mean loss: 0.114546\n",
            "accuracy: 0.141544\n",
            "2019-08-29 11:43:54.143159\n",
            "---- EPOCH 014 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007266\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 014 ****\n",
            "2019-08-29 11:43:54.796448\n",
            "----0-----\n",
            "mean loss: 0.113169\n",
            "accuracy: 0.137207\n",
            "----1-----\n",
            "mean loss: 0.113815\n",
            "accuracy: 0.135254\n",
            "----2-----\n",
            "mean loss: 0.112538\n",
            "accuracy: 0.142770\n",
            "----3-----\n",
            "mean loss: 0.117164\n",
            "accuracy: 0.147461\n",
            "----4-----\n",
            "mean loss: 0.114615\n",
            "accuracy: 0.136719\n",
            "2019-08-29 11:44:59.923624\n",
            "---- EPOCH 015 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007157\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 015 ****\n",
            "2019-08-29 11:45:00.573202\n",
            "----0-----\n",
            "mean loss: 0.112394\n",
            "accuracy: 0.138184\n",
            "----1-----\n",
            "mean loss: 0.118614\n",
            "accuracy: 0.146484\n",
            "----2-----\n",
            "mean loss: 0.113364\n",
            "accuracy: 0.143995\n",
            "----3-----\n",
            "mean loss: 0.113238\n",
            "accuracy: 0.135254\n",
            "----4-----\n",
            "mean loss: 0.114825\n",
            "accuracy: 0.137207\n",
            "2019-08-29 11:46:05.779276\n",
            "---- EPOCH 016 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007153\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 016 ****\n",
            "2019-08-29 11:46:06.423577\n",
            "----0-----\n",
            "mean loss: 0.114079\n",
            "accuracy: 0.136719\n",
            "----1-----\n",
            "mean loss: 0.116771\n",
            "accuracy: 0.147461\n",
            "----2-----\n",
            "mean loss: 0.113527\n",
            "accuracy: 0.136719\n",
            "----3-----\n",
            "mean loss: 0.113168\n",
            "accuracy: 0.143995\n",
            "----4-----\n",
            "mean loss: 0.113641\n",
            "accuracy: 0.139160\n",
            "2019-08-29 11:47:11.498967\n",
            "---- EPOCH 017 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007154\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 017 ****\n",
            "2019-08-29 11:47:12.146981\n",
            "----0-----\n",
            "mean loss: 0.113790\n",
            "accuracy: 0.145221\n",
            "----1-----\n",
            "mean loss: 0.114063\n",
            "accuracy: 0.137695\n",
            "----2-----\n",
            "mean loss: 0.113072\n",
            "accuracy: 0.136230\n",
            "----3-----\n",
            "mean loss: 0.112886\n",
            "accuracy: 0.136230\n",
            "----4-----\n",
            "mean loss: 0.118131\n",
            "accuracy: 0.146973\n",
            "2019-08-29 11:48:17.321622\n",
            "---- EPOCH 018 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007195\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 018 ****\n",
            "2019-08-29 11:48:17.990461\n",
            "----0-----\n",
            "mean loss: 0.112566\n",
            "accuracy: 0.137695\n",
            "----1-----\n",
            "mean loss: 0.115102\n",
            "accuracy: 0.143995\n",
            "----2-----\n",
            "mean loss: 0.117765\n",
            "accuracy: 0.145020\n",
            "----3-----\n",
            "mean loss: 0.114714\n",
            "accuracy: 0.136719\n",
            "----4-----\n",
            "mean loss: 0.113106\n",
            "accuracy: 0.136230\n",
            "2019-08-29 11:49:23.315221\n",
            "---- EPOCH 019 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007160\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 019 ****\n",
            "2019-08-29 11:49:23.952426\n",
            "----0-----\n",
            "mean loss: 0.111342\n",
            "accuracy: 0.138672\n",
            "----1-----\n",
            "mean loss: 0.114516\n",
            "accuracy: 0.135254\n",
            "----2-----\n",
            "mean loss: 0.117536\n",
            "accuracy: 0.145508\n",
            "----3-----\n",
            "mean loss: 0.112947\n",
            "accuracy: 0.141544\n",
            "----4-----\n",
            "mean loss: 0.113160\n",
            "accuracy: 0.139160\n",
            "2019-08-29 11:50:29.260000\n",
            "---- EPOCH 020 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007154\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 020 ****\n",
            "2019-08-29 11:50:29.900256\n",
            "----0-----\n",
            "mean loss: 0.110840\n",
            "accuracy: 0.142157\n",
            "----1-----\n",
            "mean loss: 0.117385\n",
            "accuracy: 0.146484\n",
            "----2-----\n",
            "mean loss: 0.112839\n",
            "accuracy: 0.138672\n",
            "----3-----\n",
            "mean loss: 0.111891\n",
            "accuracy: 0.139160\n",
            "----4-----\n",
            "mean loss: 0.112667\n",
            "accuracy: 0.136719\n",
            "2019-08-29 11:51:35.150703\n",
            "---- EPOCH 021 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007166\n",
            "eval classification accuracy: 0.062500\n",
            "Model saved in file: /content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/log/model.ckpt\n",
            "**** EPOCH 021 ****\n",
            "2019-08-29 11:51:36.011330\n",
            "----0-----\n",
            "mean loss: 0.115927\n",
            "accuracy: 0.147461\n",
            "----1-----\n",
            "mean loss: 0.111142\n",
            "accuracy: 0.139160\n",
            "----2-----\n",
            "mean loss: 0.113092\n",
            "accuracy: 0.140137\n",
            "----3-----\n",
            "mean loss: 0.112732\n",
            "accuracy: 0.145221\n",
            "----4-----\n",
            "mean loss: 0.111656\n",
            "accuracy: 0.137695\n",
            "2019-08-29 11:52:41.269227\n",
            "---- EPOCH 022 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007180\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 022 ****\n",
            "2019-08-29 11:52:41.926652\n",
            "----0-----\n",
            "mean loss: 0.110782\n",
            "accuracy: 0.138672\n",
            "----1-----\n",
            "mean loss: 0.110643\n",
            "accuracy: 0.137207\n",
            "----2-----\n",
            "mean loss: 0.113514\n",
            "accuracy: 0.138184\n",
            "----3-----\n",
            "mean loss: 0.112338\n",
            "accuracy: 0.145221\n",
            "----4-----\n",
            "mean loss: 0.117488\n",
            "accuracy: 0.146484\n",
            "2019-08-29 11:53:47.089738\n",
            "---- EPOCH 023 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007164\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 023 ****\n",
            "2019-08-29 11:53:47.735486\n",
            "----0-----\n",
            "mean loss: 0.112468\n",
            "accuracy: 0.137695\n",
            "----1-----\n",
            "mean loss: 0.111088\n",
            "accuracy: 0.144608\n",
            "----2-----\n",
            "mean loss: 0.113183\n",
            "accuracy: 0.138184\n",
            "----3-----\n",
            "mean loss: 0.115622\n",
            "accuracy: 0.148926\n",
            "----4-----\n",
            "mean loss: 0.111774\n",
            "accuracy: 0.138184\n",
            "2019-08-29 11:54:53.042501\n",
            "---- EPOCH 024 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007173\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 024 ****\n",
            "2019-08-29 11:54:53.699823\n",
            "----0-----\n",
            "mean loss: 0.110752\n",
            "accuracy: 0.139160\n",
            "----1-----\n",
            "mean loss: 0.116698\n",
            "accuracy: 0.148438\n",
            "----2-----\n",
            "mean loss: 0.110687\n",
            "accuracy: 0.138184\n",
            "----3-----\n",
            "mean loss: 0.112714\n",
            "accuracy: 0.139160\n",
            "----4-----\n",
            "mean loss: 0.111295\n",
            "accuracy: 0.144608\n",
            "2019-08-29 11:55:59.083490\n",
            "---- EPOCH 025 EVALUATION ----\n",
            "2\n",
            "----0-----\n",
            "(32, 1024, 3)\n",
            "----1-----\n",
            "(32, 1024, 3)\n",
            "eval mean loss: 0.007154\n",
            "eval classification accuracy: 0.062500\n",
            "**** EPOCH 025 ****\n",
            "2019-08-29 11:55:59.731663\n",
            "----0-----\n",
            "mean loss: 0.109821\n",
            "accuracy: 0.139648\n",
            "----1-----\n",
            "mean loss: 0.111750\n",
            "accuracy: 0.145221\n",
            "----2-----\n",
            "mean loss: 0.116865\n",
            "accuracy: 0.147461\n",
            "----3-----\n",
            "mean loss: 0.111819\n",
            "accuracy: 0.135742\n",
            "----4-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6ZJyqTI7qB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/tf_util.py' '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/tf_util.py'\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWy3cW2DsmVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/work/model_util.py' '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/model_util.py'\n",
        "!cp '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/models/model_util.py' '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/model_util.py'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de50ZEXKtBtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8f729a8-a7fc-4307-e609-b2d39322ab3d"
      },
      "source": [
        "!cat '/content/gdrive/My Drive/work/frustum_pointnet_radar/frustum-pointnets-master/model_util.py'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "import os\n",
            "import sys\n",
            "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
            "sys.path.append(BASE_DIR)\n",
            "import tf_util\n",
            "\n",
            "# -----------------\n",
            "# Global Constants\n",
            "# -----------------\n",
            "\n",
            "class_types={'car': 0, 'other': 1, 'pedestrian': 2, 'twowheeler': 3}\n",
            "\n",
            "NUM_FEATURES = 5   #Number of features used in input \n",
            "NUM_CLASSES = 4   #num of classes model can classify\n",
            "MODEL ='frustum_pointnets_v1'   #Model name [default: frustum_pointnets_v1]\n",
            "GPU_INDEX = 0   #GPU to use [default: GPU 0]\n",
            "LOG_DIR ='log'  #Log dir [default: log]\n",
            "NUM_POINT = 1024   #Point Number [default: 64]\n",
            "MAX_EPOCH =201   #Epoch to run [default: 201]\n",
            "BATCH_SIZE = 32   #Batch Size during training [default: 512]\n",
            "LEARNING_RATE = 0.001   #Initial learning rate [default: 0.001]\n",
            "MOMENTUM = 0.9   #Initial learning rate [default: 0.9]\n",
            "OPTIMIZER ='adam'   #adam or momentum [default: adam]\n",
            "DECAY_STEP = 200000   #Decay step for lr decay [default: 200000]\n",
            "DECAY_RATE = 0.7   #Decay rate for lr decay [default: 0.7]\n",
            "RESTORE_MODEL_PATH =None   #Restore model path e.g. log/model.ckpt [default: None]\n",
            "\n",
            "BN_INIT_DECAY = 0.5\n",
            "BN_DECAY_DECAY_RATE = 0.5\n",
            "BN_DECAY_CLIP = 0.99\n",
            "\n",
            "# TRAIN_DATASET_FEATS = np.load('train_feats.npy')\n",
            "# TRAIN_DATASET_LABELS = np.load('train_labels.npy')\n",
            "# VAL_DATASET_FEATS = np.load('val_feats.npy')\n",
            "# VAL_DATASET_LABELS = np.load('val_labels.npy')\n",
            "\n",
            "\n",
            "def placeholder_inputs(num_features, batch_size, num_point):\n",
            "    ''' Get useful placeholder tensors.\n",
            "    Input:\n",
            "        batch_size: scalar int\n",
            "        num_point: scalar int\n",
            "    Output:\n",
            "        TF placeholders for inputs and ground truths\n",
            "    '''\n",
            "    pointclouds_pl = tf.placeholder(tf.float32,\n",
            "        shape=(batch_size, num_point, num_features))\n",
            "    class_pl = tf.placeholder(tf.float32, shape=(batch_size,))\n",
            "\n",
            "    return pointclouds_pl, class_pl\n",
            "\n",
            "def get_loss(batch_size, class_label, end_points):\n",
            "    ''' Loss functions for 3D object detection.\n",
            "    Input:\n",
            "        class_label: TF int32 tensor in shape (B,)\n",
            "        end_points: dict, outputs from our model\n",
            "        \n",
            "    Output:\n",
            "        total_loss: TF scalar tensor\n",
            "            the total_loss is also added to the losses collection\n",
            "    '''\n",
            "    # Classification loss\n",
            "    # class_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\\\n",
            "        # logits=end_points['class_logits'], labels=class_label)\n",
            "    print (class_label)\n",
            "    class_label = tf.reshape(class_label, [batch_size, 1])\n",
            "    class_label = tf.one_hot(class_label, depth=4)\n",
            "    class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
            "           logits=end_points['class_logits'], labels=class_label))\n",
            "\n",
            "    tf.summary.scalar('3d classification loss', class_loss)\n",
            "    tf.add_to_collection('losses', class_loss)\n",
            "    return class_loss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4zY39H_EAx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}